{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The purpose of this anlaysis is trying to find out the products that are purchased most in the HappyDB dataset.**  \n",
    "These products are more likely to make people feel happy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I'll use KOKO, a rule-based entity extraction system, to perform the analysis.**  \n",
    "\n",
    "KOKO allows users to specify conditions of desirable entities with a declarative language (see [KOKO syntax](#koko_syntax)).  \n",
    "Those entities that obtain scores higher than a threshold are extracted.\n",
    "\n",
    "KOKO is especially suitable for entity extraction with limited evidence in the corpus (e.g. extraction of cafe names within only one or a few blogs). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The whole analysis described in this notebook comprises the following steps:**  \n",
    "\n",
    "- Data preprocessing: load HappyDB dataset and convert it to a text file as input to KOKO.\n",
    "- KOKO introduction: briefly introduce the syntax and semantics of KOKO, with an example query.\n",
    "- Entity extraction: a KOKO query is written and evaluated, extracting product names in the dataset.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first import necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud, ImageColorGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load the data and take a look at the happy moments.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load HappyDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hmid</th>\n",
       "      <th>hm</th>\n",
       "      <th>reflection</th>\n",
       "      <th>wid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27673</td>\n",
       "      <td>I went on a successful date with someone I fel...</td>\n",
       "      <td>24h</td>\n",
       "      <td>2053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27674</td>\n",
       "      <td>I was happy when my son got 90% marks in his e...</td>\n",
       "      <td>24h</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27675</td>\n",
       "      <td>I went to the gym this morning and did yoga.</td>\n",
       "      <td>24h</td>\n",
       "      <td>1936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27676</td>\n",
       "      <td>We had a serious talk with some friends of our...</td>\n",
       "      <td>24h</td>\n",
       "      <td>206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27677</td>\n",
       "      <td>I went with grandchildren to butterfly display...</td>\n",
       "      <td>24h</td>\n",
       "      <td>6227</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    hmid                                                 hm reflection   wid\n",
       "0  27673  I went on a successful date with someone I fel...        24h  2053\n",
       "1  27674  I was happy when my son got 90% marks in his e...        24h     2\n",
       "2  27675       I went to the gym this morning and did yoga.        24h  1936\n",
       "3  27676  We had a serious talk with some friends of our...        24h   206\n",
       "4  27677  I went with grandchildren to butterfly display...        24h  6227"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./cleaned_hm.csv', index_col=0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within the dataset, the most interesting part -- which is also the input to our analysis -- is the coloum of 'cleaned_hm'.  \n",
    "\n",
    "'cleaned_hm' stands for \"cleaned happy moments\". Let's take a look."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a word count first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    100922.00000\n",
       "mean         17.94577\n",
       "std          20.03303\n",
       "min           1.00000\n",
       "25%           9.00000\n",
       "50%          14.00000\n",
       "75%          21.00000\n",
       "max        1179.00000\n",
       "Name: hm, dtype: float64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hm = data[data['hm'].notnull()]\n",
    "len_count = df_hm['hm'].apply(lambda x: len(x.split()))\n",
    "len_count.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's show more detailed results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x115b86208>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAEKCAYAAADaa8itAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZlYZBxUzM6vM/wct6v93SgWxlQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x115b87828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "length_order = [\"0-4\", \"5-9\", \"10-14\", \"15-19\", \"20-24\", \"25-29\", \"30-34\", \"35-39\", \\\n",
    "                \"40-44\", \"45-49\", \">=50\"]\n",
    "length_category = len_count.apply(lambda x: length_order[min(10, int(x/5))])\n",
    "length_counts = pd.DataFrame(length_category.value_counts()).reset_index()\n",
    "length_counts.columns = ['word numbers', '# of moments']\n",
    "\n",
    "sns.barplot(x='word numbers', y='# of moments', data=length_counts, order=length_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-3226b4d2d0fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mwordcloud\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackground_color\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"white\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfont_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'../../../Library/Fonts/Arial.ttf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2700\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3600\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#, font_path='../../../Library/Fonts/FiraMono-Regular.ttf' ,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mwordcloud\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenv/python3/lib/python3.6/site-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m         \"\"\"\n\u001b[0;32m--> 556\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_generated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenv/python3/lib/python3.6/site-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    540\u001b[0m         \"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_from_frequencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenv/python3/lib/python3.6/site-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_frequencies\u001b[0;34m(self, frequencies, max_font_size)\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m                 self.generate_from_frequencies(dict(frequencies[:2]),\n\u001b[0;32m--> 407\u001b[0;31m                                                max_font_size=self.height)\n\u001b[0m\u001b[1;32m    408\u001b[0m                 \u001b[0;31m# find font sizes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m                 \u001b[0msizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayout_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenv/python3/lib/python3.6/site-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_frequencies\u001b[0;34m(self, frequencies, max_font_size)\u001b[0m\n\u001b[1;32m    435\u001b[0m                     font, orientation=orientation)\n\u001b[1;32m    436\u001b[0m                 \u001b[0;31m# get size of resulting text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m                 \u001b[0mbox_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtextsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfont\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransposed_font\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m                 \u001b[0;31m# find possible places using integral image:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m                 result = occupancy.sample_position(box_size[1] + self.margin,\n",
      "\u001b[0;32m~/.virtualenv/python3/lib/python3.6/site-packages/PIL/ImageDraw.py\u001b[0m in \u001b[0;36mtextsize\u001b[0;34m(self, text, font, spacing, direction, features)\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfont\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0mfont\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetfont\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfont\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     def multiline_textsize(self, text, font=None, spacing=4, direction=None,\n",
      "\u001b[0;32m~/.virtualenv/python3/lib/python3.6/site-packages/PIL/ImageFont.py\u001b[0m in \u001b[0;36mgetsize\u001b[0;34m(self, text, *args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgetsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m         \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfont\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morientation\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mROTATE_90\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mROTATE_270\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenv/python3/lib/python3.6/site-packages/PIL/ImageFont.py\u001b[0m in \u001b[0;36mgetsize\u001b[0;34m(self, text, direction, features)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgetsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfont\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "LIMIT_WORDS = ['happy', 'day', 'got', 'went', 'today', 'made', 'one', 'two', 'time', 'last', 'first', 'going', \\\n",
    "               'getting', 'took', 'yesterday', 'found', 'lot', 'really', 'saw', 'see', 'month', 'week', 'day', \\\n",
    "               'yesterday', 'year', 'ago', 'now', 'still', 'since', 'something', 'great', 'good', 'long', 'thing', \\\n",
    "               'toi', 'without', 'yesteri', '2s', 'toand', 'ing' ]\n",
    "\n",
    "text = ' '.join(df_hm['hm'].tolist())\n",
    "text = text.lower()\n",
    "for w in LIMIT_WORDS:\n",
    "    text = text.replace(' ' + w, '')\n",
    "    text = text.replace(w + ' ', '')\n",
    "    wordcloud = WordCloud(background_color=\"white\", font_path='../../../Library/Fonts/Arial.ttf', \\\n",
    "                          height=2700, width=3600).generate(text)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seasons that make people happy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Autumn': 0, 'Fall': 6, 'Spring': 81, 'Summer': 23, 'Winter': 6}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seasons = ['Spring', 'Summer', \"Fall\", \"Autumn\", \"Winter\"]\n",
    "\n",
    "# Check each moment, and increase the count for the mentioned season\n",
    "season_dic = dict((x,0) for x in seasons)\n",
    "tokens_hm = df_hm['hm'].apply(lambda x: x.split())\n",
    "for _, value in tokens_hm.iteritems():\n",
    "    for word in value:\n",
    "        if word in seasons:\n",
    "            season_dic[word] += 1\n",
    "            \n",
    "season_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spring is the most popular season -- as expected -- followed by Summer.  \n",
    "\n",
    "Fall and Winter have equal number of mentioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify purchasing-related moments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use Python to take a peek at all the happy moments related to purchasing behavior -- \n",
    "i.e., moments that contain keywords 'buy', 'bought' or 'purchase'.  \n",
    "\n",
    "This process could help us understand the patterns of products appearing in the happy moments, and faciliate condition specification in latter steps when we use KOKO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_clean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-f2c95df6c225>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnum_moments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnum_moments\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mdata_clean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Happy moments involving purchasing:\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_moments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'buy'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_clean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mor\u001b[0m        \u001b[0;34m'bought'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_clean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mor\u001b[0m        \u001b[0;34m'purchase'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_clean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_clean' is not defined"
     ]
    }
   ],
   "source": [
    "num_moments = 100\n",
    "assert (num_moments < data_clean.size)\n",
    "print('Happy moments involving purchasing:\\n')\n",
    "for i in range(0, num_moments):\n",
    "    if 'buy' in data_clean.iloc[i] or \\\n",
    "       'bought' in data_clean.iloc[i] or \\\n",
    "       'purchase' in data_clean.iloc[i]:\n",
    "       print(\"{}: {}\".format(i, data_clean.iloc[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='koko_syntax'></a>\n",
    "# 2. Introduction to KOKO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using KOKO to extract products from HappyDB, I'll give a brief introduction of KOKO's query language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here's am example query *Q_prod* that I'll use to extract products.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extract \"Ngrams(1,1)\" x from \"./happydb.txt\" if\n",
      "\t\t   (\"bought a new\" x {0.01}) or\n",
      "\t\t   (\"bought a few\" x {0.01}) or\n",
      "\t\t   (\"bought some\" x {0.01}) or\n",
      "\t\t   (\"bought a\" x {0.01}) or\n",
      "\t\t   (x \"I bought\" {0.01}) or\t\t   \n",
      "\t\t   (\"purchased a new\" x {0.01}) or\n",
      "\t\t   (\"purchased a few\" x {0.01}) or\n",
      "\t\t   (\"purchased some\" x {0.01}) or\n",
      "\t\t   (\"purchased a\" x {0.01}) or\n",
      "\t\t   (x \"I purchased\" {0.01})\n",
      "with threshold 0.2\n",
      "excluding (str(x) matches \".*(new|NEW|few).*\")\n",
      "excluding (str(x) matches \".*(,|\\.|;|!|\\$|\\(|\\)|-).*\")\n",
      "excluding (str(x) matches \".*[0-9]+.*\")\n",
      "excluding (str(x) matches \".*(and|or|so|the|this|that).*\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('./products.koko', 'r') as query:\n",
    "    print(query.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand *Q_prod*, let's take a look at the syntax of KOKO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='koko_syntax'></a>\n",
    "## KOKO syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(**extract** ⟨keyword⟩ x **from** ⟨document name⟩ **if**  \n",
    "⟨condition⟩  \n",
    "(**with threshold** ⟨threshold⟩)  \n",
    "[**excluding** ⟨e-condition⟩]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where the conditions are defined as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*⟨condition⟩ ::= ⟨condition⟩ or ⟨condition⟩ |*  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "              *(x {⟨string⟩} ⟨weight⟩) |*  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "              *(x ⟨string⟩ ⟨weight⟩) |*  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "              *(x near ⟨string⟩ ⟨weight⟩) |*  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "              *(str(x) matches⟨pattern⟩) |*  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "              *(str(x) [contains|mentions] {⟨string⟩} ⟨weight⟩) |*  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "              *(str(x) [contains|mentions] ⟨string⟩ ⟨weight⟩)*\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  \n",
    "*⟨weight⟩ ::= empty | number in [0,1]*\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  \n",
    "*⟨threshold⟩ ::= number in [0,1]*\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  \n",
    "*⟨pattern⟩::= ⟨regular expression⟩*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifically, a KOKO query will extract entities *x* of type ⟨keyword⟩ from a document ⟨document name⟩, \n",
    "if the score of *x* exceeds the treshold ⟨threshold⟩.  \n",
    "\n",
    "The score is computed as the cumulative weights of conditions that are satisfied by *x* in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Let's use the query *Q_prod* presented above as an example**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In *Q_prod*, the ⟨keyword⟩ is \"Ngrams(1,1)\", which means all the one-gram in the document.  \n",
    "We can also use \"Ents\" for named entities, or \"Nps\" for noun phrases.  \n",
    "\n",
    "There are twelve conditions in ⟨condition⟩. For example, (\"bought a new\" x {0.01}) means that an entity *x* with a preceding string of \"bought a new\" will have its score increased by 0.01 -- i.e., the weight of the condition.  \n",
    "\n",
    "And the first \"excluding\" keyword specifies that the matching entities should not be any word containing \"new\", \"NEW\", or \"few\" -- we are more interested in \"car\", for instance, than \"a new car\" or \"a few cars\".  \n",
    "Other \"excluding\" conditions have a similar role."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Entity extraction with KOKO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to run the query for product extraction.  \n",
    "\n",
    "First, we need to install the KOKO package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install KOKO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install KOKO locally, simply run the following command:\n",
    "\n",
    "    pip install pykoko"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate plain-text happy moments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KOKO queries take texts as input. Let's generate plain-text happy moments for query evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plain-text happy moments are generated!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# Read the happyDB sample file\n",
    "with open('./happydb.txt', 'w') as ofile:\n",
    "    for i in range(0, data_clean.size-1):    \n",
    "        ofile.write(\"\\t\" + str(data_clean.iloc[i]) + '\\n')\n",
    "        \n",
    "print(\"Plain-text happy moments are generated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run KOKO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After KOKO is installed, we can run the example query *Q_prod*.  \n",
    "\n",
    "Considering the size of the dataset, it might take several minutes to get results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding models\n",
      "Creating QueryExpander for: en\n",
      "Embeddings file not found: /Users/chen/.virtualenv/python3/lib/python3.6/site-packages/koko/../embeddings/commoncrawl.840B.300d.txt\n",
      "Ontology file not found: /Users/chen/.virtualenv/python3/lib/python3.6/site-packages/koko/../coffee_ontology.txt\n",
      "Creating QueryExpander for: ja\n",
      "Embeddings file not found: /Users/chen/.virtualenv/python3/lib/python3.6/site-packages/koko/../embeddings/japanese_noun_verb_embedding_vectors.txt\n",
      "Ontology file not provided\n",
      "Done loading embedding models\n",
      "Parsed query: extract \"./happydb.txt\" Ngrams(1,1) from \"x\" if\n",
      "\t(\"bought a new\" x { 0.01 }) or\n",
      "\t(\"bought a few\" x { 0.01 }) or\n",
      "\t(\"bought some\" x { 0.01 }) or\n",
      "\t(\"bought a\" x { 0.01 }) or\n",
      "\t(x \"I bought\" { 0.01 }) or\n",
      "\t(\"purchased a new\" x { 0.01 }) or\n",
      "\t(\"purchased a few\" x { 0.01 }) or\n",
      "\t(\"purchased some\" x { 0.01 }) or\n",
      "\t(\"purchased a\" x { 0.01 }) or\n",
      "\t(x \"I purchased\" { 0.01 })   \n",
      "with threshold 0.20\n",
      "excluding\n",
      "\t(str(x) matches \".*(new|NEW|few).*\")\n",
      "\t(str(x) matches \".*(,|\\.|;|!|\\$|\\(|\\)|-).*\")\n",
      "\t(str(x) matches \".*[0-9]+.*\")\n",
      "\t(str(x) matches \".*(and|or|so|the|this|that).*\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import koko\n",
    "\n",
    "koko.run('./products.koko')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results contain certain noise, such as \"When\", \"pair\". But most entries are relevant.\n",
    "\n",
    "**It seems that expensive purchase, such as cars, houses or laptops, are mentioned most in HappyDB.**  \n",
    "\n",
    "Well, this makes sense. Expensive purchase is often for products people long for but could only afford after saving money for an extended period of time.  \n",
    "No wonder such purchase makes people happy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use spaCy or Google NLP for document parsing, instead of KOKO's default parser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here's an example of using spaCy as the parser**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 2017-09-22 15:15:48,160 - Loading SpaCy English models\n",
      "INFO 2017-09-22 15:15:48,160 - Loading SpaCy English models\n",
      "INFO 2017-09-22 15:15:48,160 - Loading SpaCy English models\n",
      "INFO 2017-09-22 15:15:48,160 - Loading SpaCy English models\n",
      "INFO 2017-09-22 15:15:50,853 - Done\n",
      "INFO 2017-09-22 15:15:50,853 - Done\n",
      "INFO 2017-09-22 15:15:50,853 - Done\n",
      "INFO 2017-09-22 15:15:50,853 - Done\n",
      "Parsed query: extract \"/Users/chen/Research/Playground/Github_Playground/happydb/data/happyDB_clean.txt\" Ngrams(1,1) from \"x\" if\n",
      "\t(\"bought a new\" x { 0.01 }) or\n",
      "\t(\"bought a few\" x { 0.01 }) or\n",
      "\t(\"bought some\" x { 0.01 }) or\n",
      "\t(\"bought a\" x { 0.01 }) or\n",
      "\t(x \"I bought\" { 0.01 }) or\n",
      "\t(\"purchased a new\" x { 0.01 }) or\n",
      "\t(\"purchased a few\" x { 0.01 }) or\n",
      "\t(\"purchased some\" x { 0.01 }) or\n",
      "\t(\"purchased a\" x { 0.01 }) or\n",
      "\t(x \"I purchased\" { 0.01 })   \n",
      "with threshold 0.20\n",
      "excluding\n",
      "\t(str(x) matches \".*(new|NEW|few).*\")\n",
      "\t(str(x) matches \".*(,|\\.|;|!|\\$|\\(|\\)|-).*\")\n",
      "\t(str(x) matches \".*[0-9]+.*\")\n",
      "\t(str(x) matches \".*(and|or|so|the|this|that).*\")\n",
      "\n",
      "\n",
      "Results:\n",
      "\n",
      "Entity name                                        Entity score\n",
      "===============================================================\n",
      "                                                   1.000000\n",
      "car                                                1.000000\n",
      "When                                               1.000000\n",
      "pair                                               0.470000\n",
      "house                                              0.430000\n",
      "laptop                                             0.420000\n",
      "bike                                               0.360000\n",
      "smartphone                                         0.330000\n",
      "video                                              0.320000\n",
      "yesterday                                          0.310000\n",
      "month                                              0.290000\n",
      "Computer                                           0.280000\n",
      "phone                                              0.240000\n",
      "dress                                              0.240000\n",
      "Bicycle                                            0.240000\n",
      "home                                               0.220000\n",
      "nice                                               0.210000\n",
      "Air                                                0.200000\n",
      "game                                               0.200000\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "koko.run('./products.koko', doc_parser='spacy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are slightly different. But the entities extracted are identical except \"TV\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a quick and preliminary analysis of the HappyDB dataset. In the analysis:\n",
    "\n",
    "- I tried to extract the products that tend to make people happy.\n",
    "\n",
    "- I introduced KOKO, an entity extraction system with a declarative rule-based specification language.\n",
    "\n",
    "- I showed how to write KOKO queries to concisely specify desirable entities, and use KOKO runtime to extract these entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
